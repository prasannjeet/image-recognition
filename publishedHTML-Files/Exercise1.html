
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Exercise 1 - Visualization and Cross-Validation</title><meta name="generator" content="MATLAB 9.4"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2018-05-14"><meta name="DC.source" content="Exercise1.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Exercise 1 - Visualization and Cross-Validation</h1><!--introduction--><p>Submitted by <b>Prasannjeet Singh</b></p><p>
  <link rel="stylesheet" type="text/css" href="../Data/layout.css">
</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Q1. Using fitctree()</a></li><li><a href="#2">Q2. Decision Boundary</a></li><li><a href="#4">Q3. k-Fold Cross Validation</a></li><li><a href="#6">Q4. Zero training error decision tree.</a></li></ul></div><h2 id="1">Q1. Using fitctree()</h2><pre class="codeinput">load <span class="string">Data/data1.mat</span>;
mdl = fitctree(X,y);
view(mdl);
view(mdl, <span class="string">'Mode'</span>,<span class="string">'graph'</span>);
snapnow;
hTree=findall(0,<span class="string">'Tag'</span>,<span class="string">'tree viewer'</span>);
close(hTree);
</pre><pre class="codeoutput">
Decision tree for classification
 1  if x1&lt;13.5 then node 2 elseif x1&gt;=13.5 then node 3 else 0
 2  if x2&lt;10.5 then node 4 elseif x2&gt;=10.5 then node 5 else 1
 3  class = 0
 4  if x2&lt;-35 then node 6 elseif x2&gt;=-35 then node 7 else 1
 5  class = 0
 6  class = 0
 7  if x2&lt;-3.5 then node 8 elseif x2&gt;=-3.5 then node 9 else 1
 8  class = 1
 9  if x2&lt;-2 then node 10 elseif x2&gt;=-2 then node 11 else 1
10  class = 0
11  class = 1

</pre><img vspace="5" hspace="5" src="Exercise1_01.png" alt=""> <h2 id="2">Q2. Decision Boundary</h2><p>The decision boundary was calculated by using the inbuilt method <b>predict()</b> which predicts the answer based on the decision tree created earlier. Using predict, all the values were calculated for a graph with both the axes (-50,50) and stored in a matrix. 50 was chosen manually by observing the scatter of the input data X. Later <b>contourf()</b> was used in the matrix to plot the decision boundary.</p><pre class="codeinput">v = repmat(-50:50,[101,1]);
<span class="comment">% Creating a two-columned matrix 'z', that contains all the integer points</span>
<span class="comment">% in a square graph of axes (-50,50). Next, using 'predict' to calculate</span>
<span class="comment">% predicted answers for all the points in 'z'.</span>
z =  [v(:) repmat([-50:50]',[101,1])];
bMatrix = predict(mdl,z);
bMatrix = vec2mat(bMatrix,101);
hFig = figure(2);
contourf(-50:50,-50:50,bMatrix,1);
title(<span class="string">'Decision Boundary for Decision Tree'</span>);
legend(<span class="string">'Yellow - 1 | Blue - 0'</span>);
snapnow; close(hFig);
</pre><img vspace="5" hspace="5" src="Exercise1_02.png" alt=""> <p><b>What is the characteristic for the decision boundary of a tree? Conceptually, can you obtain any kind of decision boundary using decision trees?</b></p><p>In case of a decision tree, the algorithm tries to split the entire dataset in two parts based on any one feature, and the feature that will be chosen depends upon the greedy algorithm. Therefore, in our example which has a dataset with two features, the dataset with be split either horizontally or verticaly.</p><p>It is evident that regardless of the chosen axis, the algorithm can only split the dataset parallel to the x OR y axis at any iteration. This is a unique characteristic that can be observed in a decision boundary for a decision tree. Therefore, we can conclude that <i>any kind of</i> decision boundary, like what we saw in the assignment 1 is not possible in this case, and the boundaries will only consist of straight lines parallel to the x and y axis, as clearly seen in the above figure. However, it should be noted that <b>if the dataset is huge</b> and <b>if the minimum parent size is very small</b>, the decision boundary might appear to be of <i>any</i> shape, but a closer look (zoomed-in) will reveal that the boundaries are actually parallel to either x or y axis.</p><h2 id="4">Q3. k-Fold Cross Validation</h2><p>For choosing a value of 'k', following thoughts were taken into consideration:</p><div><ol><li><i>k</i> is preferably a divisor of number of training sets (n).</li><li><i>k</i> is considerably small such that there is enough training data.</li></ol></div><p>Since <i>n</i> is 60 in this example, k was taken as <i>10</i>.</p><p>The function <b>kfoldLoss()</b> calculates the classification error, specified by 'LossFun','classiferror'. It is the weighted fraction of misclassified observations, with the equation:</p><p><img src="Exercise1_eq03196131173537897014.png" alt="$$L = \sum_{j=1}^n {w_j I \big\{\widehat{y_j} \neq y_j \big\} }$$"></p><p>Where <img src="Exercise1_eq02832347711439461536.png" alt="$\widehat{y}_j$"> is the class label corresponding to the class with the maximal posterior probability. I{x} is the indicator function. (Taken from <a href="https://se.mathworks.com/help/stats/classificationpartitionedmodel.kfoldloss.html#bswic2v-2">this link</a> of the <b>Matlab Documentation</b>). Therefore, the final value can be multiplied with <i>n</i> (60 in our case) to get the total number of misclassifications.</p><pre class="codeinput">k = 10;
cvmodel = crossval(mdl,<span class="string">'KFold'</span>,k);
WeightedLoss = kfoldLoss(cvmodel,<span class="string">'lossfun'</span>,<span class="string">'classiferror'</span>,<span class="string">'mode'</span>,<span class="string">'average'</span>);
<span class="comment">% round() is used below, as sometimes division by 6 can create fractional</span>
<span class="comment">% values, and multiplying it by 60 again does not completely remove the</span>
<span class="comment">% fractional part.</span>
ClassificationError = round(WeightedLoss*60)
</pre><pre class="codeoutput">
ClassificationError =

    10

</pre><p>Above we can see the total classification error obtained with 10-Fold cross validation. Note that the value is prone to change with every execution of the function, as the datasets are permuted before we perform k-Fold.</p><h2 id="6">Q4. Zero training error decision tree.</h2><pre class="codeinput">mdl2 = fitctree(X,y,<span class="string">'MinParentSize'</span>,1);
bMatrix = predict(mdl2,z);
bMatrix = vec2mat(bMatrix,101);
</pre><p>Visualizing as a <b>Graph</b>:</p><pre class="codeinput">view(mdl2, <span class="string">'Mode'</span>,<span class="string">'graph'</span>);
snapnow;
hTree=findall(0,<span class="string">'Tag'</span>,<span class="string">'tree viewer'</span>);
close(hTree);
</pre><img vspace="5" hspace="5" src="Exercise1_03.png" alt=""> <p>Visualizing by the <b>Decision Boundary</b>:</p><pre class="codeinput">hFig = figure(2);
contourf(-50:50,-50:50,bMatrix,1);
title(<span class="string">'Decision Boundary for MinParent Size = 1'</span>);
legend(<span class="string">'Yellow - 1 | Blue - 0'</span>);
snapnow; close(hFig);
</pre><img vspace="5" hspace="5" src="Exercise1_04.png" alt=""> <p><b>k-Fold Cross Validation</b></p><p>k Value will be kept the same.</p><pre class="codeinput">cvmodel = crossval(mdl2,<span class="string">'KFold'</span>,k);
WeightedLoss = kfoldLoss(cvmodel,<span class="string">'lossfun'</span>,<span class="string">'classiferror'</span>,<span class="string">'mode'</span>,<span class="string">'average'</span>);
<span class="comment">% round() is used below, as sometimes division by 6 can create fractional</span>
<span class="comment">% values, and multiplying it by 60 again does not completely remove the</span>
<span class="comment">% fractional part.</span>
ClassificationError = round(WeightedLoss*60)
</pre><pre class="codeoutput">
ClassificationError =

    12

</pre><p>In the test run, the total classification errors in case of minimum parent size as the default value (10) was <b>9</b>, and in case of minimum parent size as 1 was <b>13</b>. (Note that the values may slightly change everytime this file is run).</p><p>The fact that total error in case of a decision tree with zero training error is more than the former is very much expected, as this comes under the category of <b>overfitting</b>, and as we have seen in various questions in previous exercises, overfitting always leads to high training errors. Although we do not have a training data in this case, k-Fold converts parts of the test data into training data in each iteration, therefore this can also, in a way, be considered as training error, which is higher in this case of overfitting.</p><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2018a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Exercise 1 - Visualization and Cross-Validation
% Submitted by *Prasannjeet Singh*
%
% <html>
%   <link rel="stylesheet" type="text/css" href="../Data/layout.css">
% </html>
%
%% Q1. Using fitctree()

load Data/data1.mat;
mdl = fitctree(X,y);
view(mdl);
view(mdl, 'Mode','graph');
snapnow;
hTree=findall(0,'Tag','tree viewer');
close(hTree);

%% Q2. Decision Boundary
% The decision boundary was calculated by using the inbuilt method
% *predict()* which predicts the answer based on the decision tree created
% earlier. Using predict, all the values were calculated for a graph with
% both the axes (-50,50) and stored in a matrix. 50 was chosen manually by
% observing the scatter of the input data X. Later *contourf()* was used in
% the matrix to plot the decision boundary.

v = repmat(-50:50,[101,1]);
% Creating a two-columned matrix 'z', that contains all the integer points
% in a square graph of axes (-50,50). Next, using 'predict' to calculate
% predicted answers for all the points in 'z'.
z =  [v(:) repmat([-50:50]',[101,1])];
bMatrix = predict(mdl,z);
bMatrix = vec2mat(bMatrix,101);
hFig = figure(2);
contourf(-50:50,-50:50,bMatrix,1);
title('Decision Boundary for Decision Tree');
legend('Yellow - 1 | Blue - 0');
snapnow; close(hFig);

%%
% *What is the characteristic for the decision boundary of a tree?
% Conceptually, can you obtain any kind of decision boundary using decision
% trees?*
%
% In case of a decision tree, the algorithm tries to split the entire
% dataset in two parts based on any one feature, and the feature that will
% be chosen depends upon the greedy algorithm. Therefore, in our example
% which has a dataset with two features, the dataset with be split either
% horizontally or verticaly.
%
% It is evident that regardless of the chosen axis, the algorithm can only
% split the dataset parallel to the x OR y axis at any iteration. This is a
% unique characteristic that can be observed in a decision boundary for a
% decision tree. Therefore, we can conclude that _any kind of_ decision
% boundary, like what we saw in the assignment 1 is not possible in this
% case, and the boundaries will only consist of straight lines parallel to
% the x and y axis, as clearly seen in the above figure. However, it should
% be noted that *if the dataset is huge* and *if the minimum parent size is
% very small*, the decision boundary might appear to be of _any_ shape, but
% a closer look (zoomed-in) will reveal that the boundaries are actually
% parallel to either x or y axis.

%% Q3. k-Fold Cross Validation
% For choosing a value of 'k', following thoughts were taken into
% consideration:
%
% # _k_ is preferably a divisor of number of training sets (n).
% # _k_ is considerably small such that there is enough training data.
%
% Since _n_ is 60 in this example, k was taken as _10_.
%
% The function *kfoldLoss()* calculates the classification error, specified
% by 'LossFun','classiferror'. It is the weighted fraction of misclassified
% observations, with the equation:
%
% $$L = \sum_{j=1}^n {w_j I \big\{\widehat{y_j} \neq y_j \big\} }$$
%
% Where $\widehat{y}_j$ is the class label corresponding to the class with
% the maximal posterior probability. I{x} is the indicator function. (Taken
% from
% <https://se.mathworks.com/help/stats/classificationpartitionedmodel.kfoldloss.html#bswic2v-2
% this link> of the *Matlab Documentation*). Therefore, the final value can
% be multiplied with _n_ (60 in our case) to get the total number of
% misclassifications.



k = 10;
cvmodel = crossval(mdl,'KFold',k);
WeightedLoss = kfoldLoss(cvmodel,'lossfun','classiferror','mode','average');
% round() is used below, as sometimes division by 6 can create fractional
% values, and multiplying it by 60 again does not completely remove the
% fractional part.
ClassificationError = round(WeightedLoss*60)

%%
% Above we can see the total classification error obtained with 10-Fold
% cross validation. Note that the value is prone to change with every
% execution of the function, as the datasets are permuted before we perform
% k-Fold.

%% Q4. Zero training error decision tree.

mdl2 = fitctree(X,y,'MinParentSize',1);
bMatrix = predict(mdl2,z);
bMatrix = vec2mat(bMatrix,101);

%%
% Visualizing as a *Graph*:

view(mdl2, 'Mode','graph');
snapnow;
hTree=findall(0,'Tag','tree viewer');
close(hTree);

%%
% Visualizing by the *Decision Boundary*:
hFig = figure(2);
contourf(-50:50,-50:50,bMatrix,1);
title('Decision Boundary for MinParent Size = 1');
legend('Yellow - 1 | Blue - 0');
snapnow; close(hFig);

%%
% *k-Fold Cross Validation*
%
% k Value will be kept the same.

cvmodel = crossval(mdl2,'KFold',k);
WeightedLoss = kfoldLoss(cvmodel,'lossfun','classiferror','mode','average');
% round() is used below, as sometimes division by 6 can create fractional
% values, and multiplying it by 60 again does not completely remove the
% fractional part.
ClassificationError = round(WeightedLoss*60)

%%
% In the test run, the total classification errors in case of minimum
% parent size as the default value (10) was *9*, and in case of minimum
% parent size as 1 was *13*. (Note that the values may slightly change
% everytime this file is run).
%
% The fact that total error in case of a decision tree with zero training
% error is more than the former is very much expected, as this comes under
% the category of *overfitting*, and as we have seen in various questions
% in previous exercises, overfitting always leads to high training errors.
% Although we do not have a training data in this case, k-Fold converts
% parts of the test data into training data in each iteration, therefore
% this can also, in a way, be considered as training error, which is higher
% in this case of overfitting.

##### SOURCE END #####
--></body></html>